---
title: "Тюнинг сетевого стека (nic perfomance)"
date: 2019-07-11 00:00:51 +0000
categories:
  - linux
  - nic
tags:
  - tune nic perfomance
---


##### Рекоммендации :
 - отключить HT
 - привязывать сетевую карту к одному процессору (numa) 
 
##### Повышение производительности сетевого стека linux.

* Что будем тюнить
  * CPU 
  * NIC
  * Soft interrupt issued by a device driver
  * Kernel buffer
  * The network layer (IP, TCP or UDP)



####  CPU 
```
 apt install cpufrequtils
 cpufreq-set -g performance
```
####  NIC 

Загрузка моделя ixgbe
```
modprobe ixgbe
/etc/modprobe.d/ixgbe.conf 
 options ixgbe IntMode=2,2  RSS=6,6  VMDQ=0,0 InterruptThrottleRate=1,1 allow_unsupported_sfp=1,1 
```
  > IntMode 2  - режим MSI-X , нужен для поддержки multiqueue
  >
  > RSS   - сколько использовать очередей 
  >
  > VMDQ отключаем, так как ненужен.
  >
  > InterruptThrottleRate подбирает динамически  кол. прерываний  в сек.
  >
  > DCA defaylt 1 (enable)
  >
  > MQ default 1  (enable) нужно включить для поддержки RSS
  >
  > max_vfs  default 0 
  >
  > через запятую указываем значение для каждого порта,если четыре то 1,1,1,1
  
##### Отключение оптимизации ядром
 ```
  ethtool -K eth0 gso off gro off tso off
```
##### Отключение системеное управление прерываний и передаем контроль NAPI.
``` 
 ethtool -C eth0 adaptive-rx off
```
##### Настройка размера очередей приёма rx-buffers-increase 

 *Узнать размер буфера*
```
 ethtool -g eth0  

```
*Установить размер буфера*
```
  ethtool -G eth0 rx 4096
```
##### Размер очереди пакетов
```
  ip link set  eth0 txqueuelen 10000
 ```
 
##### Так как мы настроили распределение очередей по превываниям (msi-x , rss ), то теперь их можно распределить по ядрам
 smp_affinity, c драйверами идет скрипт set_irq_affinity.  с привязкой к одному cpu , посмотерть номер ядра к какому cpu относится lscpu -p
 ```
  set_irq_affinity 1,2,4,6,8,10,12,14,16,18,20,22 -x eth0  
 ```
 p.s.остановить демон балансировки service irqbalance stop
 
 
##### Для распределения программынх прерываний(pppoe) нужно задействовать RPS(rps_cups) ,и задействум механизм упрвления потоками RFS(rps_flow_cnt)
 
```
echo "0" > /sys/class/net/ens2f0/queues/rx-0/rps_cpus
echo "1" > /sys/class/net/ens2f0/queues/rx-0/rps_cpus
echo "2" > /sys/class/net/ens2f0/queues/rx-1/rps_cpus
echo "4" > /sys/class/net/ens2f0/queues/rx-2/rps_cpus
echo "8" > /sys/class/net/ens2f0/queues/rx-3/rps_cpus
echo "10" > /sys/class/net/ens2f0/queues/rx-4/rps_cpus
echo "20" > /sys/class/net/ens2f0/queues/rx-5/rps_cpus
echo "40" > /sys/class/net/ens2f0/queues/rx-6/rps_cpus
echo "80" > /sys/class/net/ens2f0/queues/rx-7/rps_cpus
sysctl -w net.core.rps_sock_flow_entries=32768
echo 2048 > /sys/class/net/ens2f0/queues/rx-0/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-1/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-2/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-3/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-4/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-5/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-6/rps_flow_cnt
echo 2048 > /sys/class/net/ens2f0/queues/rx-7/rps_flow_cnt
```
 
#####   budget — это весь доступный бюджет, который будет разделён на все доступные NAPI-структуры, зарегистрированные на этот CPU. 
   по умолчанию в некторых системах он равен 300 , рекомендуется установить 600 для сетей 10Gb/s и выше
   ```
   sysctl -w net.core.netdev_budget=600
  ```
####  Kernel buffer  
  ```
# CORE settings (mostly for socket and UDP effect)
# set maximum receive socket buffer size, default 131071 
net.core.rmem_max = 524287 
# set maximum send socket buffer size, default 131071
net.core.wmem_max = 524287 
# set default receive socket buffer size, default 65535
net.core.rmem_default = 524287 
# set default send socket buffer size, default 65535
net.core.wmem_default = 524287 
# set maximum amount of option memory buffers, default 10240
net.core.optmem_max = 524287 
```
 [recommendet for 10G](https://downloadmirror.intel.com/5874/eng/README.txt)
 
#### Tcp Udp kernel tune
 
 *TCP buffer*
 IPV4 specific settings
 
 *turn TCP timestamp support off, default 1, reduces CPU use*
 
net.ipv4.tcp_timestamps = 0 
 *turn SACK support off, default on*
  on systems with a VERY fast bus -> memory interface this is the big gainer

net.ipv4.tcp_sack = 0 
 *set min/default/max TCP read buffer, default 4096 87380 174760*

net.ipv4.tcp_rmem = 10000000 10000000 10000000 
 *set min/pressure/max TCP write buffer, default 4096 16384 131072*

net.ipv4.tcp_wmem = 10000000 10000000 10000000 
 *set min/pressure/max TCP buffer space, default 31744 32256 32768*

net.ipv4.tcp_mem = 10000000 10000000 10000000 

[recommendet for 10G](https://downloadmirror.intel.com/5874/eng/README.txt)

*UDP buffer size
UDP generally doesn’t need tweaks to improve performance, but you can increase the UDP buffer size if UDP packet losses are happening.*
```
$ sysctl net.core.rmem_max
```

*Miscellaneous tweaks
IP ports: net.ipv4.ip_local_port_range shows all the ports available for a new connection. If no port is free, the connection gets cancelled. Increasing this value helps to prevent this problem.
default net.ipv4.ip_local_port_range = 32768	60999*

```
sysctl -w net.ipv4.ip_local_port_range=’20000 60000’
```

*TCP SACK*
```
sysctl -w net.ipv4.tcp_sack=0
```

*TCP FIN timeout  (def 60)*
```
sysctl -w net.ipv4.tcp_fin_timeout = 20
```

*TCP timestamp*

```
sysctl -w net.ipv4.tcp_timestamps=0
```
*Как долго держать запись о поднятой сессии (def 4 days)*
```
net.netfilter.nf_conntrack_tcp_timeout_established = 900
```

*Добавляем колличество полуоткрытых соединений *

```
$sysctl -w net.core.somaxconn=2048
```

