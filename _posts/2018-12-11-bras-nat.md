---
title: "Встала задача перености отдельно стоящего nat сервера на  BRAS сервер "
date: 2018-12-11 09:00:00 +0000
categories:
  - NAT
tags:
  - accel-ppp
  - debian
  - persistent
sidebar:
  - title: "Debian NAT"
    text: "настройка nat"
---

На accel-ppp был отключен connection tracking для абонентских сетей
```bash
#iptables -t raw -L -n
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  192.168.0.0/24       0.0.0.0/0           
ACCEPT     all  --  10.2.1.0/24          0.0.0.0/0           
ACCEPT     all  --  0.0.0.0/0            192.168.0.0/24      
ACCEPT     all  --  0.0.0.0/0            10.2.1.0/24         
NOTRACK    all  --  0.0.0.0/0            0.0.0.0/0           

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  0.0.0.0/0            192.168.0.0/24      
ACCEPT     all  --  0.0.0.0/0            10.2.1.0/24         
ACCEPT     all  --  192.168.0.0/24       0.0.0.0/0           
ACCEPT     all  --  10.2.1.0/24          0.0.0.0/0           
NOTRACK    all  --  0.0.0.0/0            0.0.0.0/0     
```
Нужно либо переписать правило NOTRACK all 0.0.0.0/0 на ACCEPT,  либо удалить его .  
Оптимальнее всего убрать все правила из цепочки RAW.

выполним через консоль:

```bash
iptables -t raw -F PREROUTING
iptables -t raw -F OUTPUT
```
и сохраним изменеия в скрипте который подгружает правила при старте. 


Как проверить сколько у нас записей в таблице connaction tracking ,
это делается с помощью пакета conntrack -L | wc -l 

Что будем тюнить :

1)
![]({{ site.baseurl }}/uploads/Figure-1-Data-receiving-process.jpg "nic tune")
https://opensourceforu.com/2016/10/network-performance-monitoring/

2)
cpu governor

best practics : https://fasterdata.es.net/host-tuning/linux/

best practics : https://habr.com/company/mailru/blog/314168/


Если сервер работает только маршрутизатором, то тюнинг TCP стека особого значения не имеет, кроме тюнинга параметров касаемых производительносит сетевой карты (буфера, очередей, распределения прерывайни и т.д.)

В нашем случае сервер будет выполнять задачу NAT и нагрузка ляжет на процессор для оптимизации таблицы необходимо будет править все параметры указанные .

# 1) Interrupt Coalescence 
После того как сетевой адаптер получает пакеты, драйвер устройства выдает одно жесткое прерывание, за которым следуют программные
прерывания (обрабатываются NAPI). Объединение прерываний - это количество пакетов, которые сетевой адаптер получает до выдачи
жесткого прерывания. Изменение значения для быстрого прерывания может привести к большим накладным расходам и, следовательно, к
снижению производительности, но высокое значение может привести к потере пакетов. По умолчанию ваши настройки находятся в режиме
адаптивной ИС, который автоматически балансирует значение в соответствии с сетевым трафиком. Но поскольку новые ядра используют
NAPI, что делает быстрые прерывания гораздо менее затратными (с точки зрения производительности), вы можете отключить эту
функцию.
```bash
ethtool -c eth0
ethtool -C eth0 adaptive-rx off

ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 0
sample-interval: 0
```

# 2) Пауза кадров 
Кадр паузы - это длительность паузы в миллисекундах, которую адаптер выдает сетевому коммутатору для остановки отправки пакетов, если кольцевой буфер заполнен. Если включен режим паузы, потеря пакетов будет минимальной.
```bash 
ethtool -a eth0
Pause parameters for eth0:
Autonegotiate:	off
RX:		off
TX:		off
ethtool -A eth0   tx on rx on
```


# 3)  изменение параметров через ethtool.  часть статей готоврит что сетевые карты затрудительно работают 
# с этими включенными  параметрами. 
ethtool -K eth1 tx off rx off sg off tso off ufo off gso off lro off rxvlan off txvlan off

# 4)  Некоторые сетевые карты и их драйверы поддерживают настройку размера очереди приёма. Конкретная реализация зависит от     
оборудования, но, к счастью, ethtool обеспечивает стандартный метод настройки. Увеличение размера позволяет предотвратить
отбрасывание сетевых данных при большом количестве входящих фреймов. Правда, данные ещё могут быть отброшены на уровне ПО, так
что для полного исключения или снижения отбрасывания необходимо будет провести дополнительную настройку.

Проверка текущего размера очереди сетевой карты с помощью ethtool –g:

```bash
$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:   4096
RX Mini:  0
RX Jumbo: 0
TX:   4096
Current hardware settings:
RX:   512
RX Mini:  0
RX Jumbo: 0
TX:   512
```

Выходные данные показывают, что оборудование поддерживает 4096 дескрипторов приёма и передачи, но в данный момент используется 512.

Увеличим размер каждой очереди до 4096:
```bash 
$ sudo ethtool -G eth0 rx 4096
```
# 5)  количесвто очередей.    можно настривать разными способами.  если сетевая карта поддерживает передачу параметров через ethtool - l то вывод будет 
```bash
ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX:		0
TX:		0
Other:		1
Combined:	63
Current hardware settings:
RX:		0
TX:		0
Other:		1
Combined:	24
```
Для того, чтобы равномерно распределить прерывания на сетевом интерфейсе необходимо изменить количество очередей.
Например, у нас имеется 6-ти ядерный процессор, а сетевая карта поддерживает 8 очередей.
Очень важно! Для того чтобы установить именно то значение, которое требуется, необходимо сбросить значение количества очередей до 1. Так как ethtool -L прибавляет количество очередей.http://docs.carbonsoft.ru/
```bash
ethtool -L eth1 rx 1 tx 1
ethtool -L eth1 rx 5 tx 5
```
Получаем (в поле Combined):

```bash
ethtool -l eth1
Channel parameters for eth1:
Pre-set maximums:
RX:		16
TX:		16
Other:		1
Combined:	16
Current hardware settings:
RX:		0
TX:		0
Other:		1
Combined:	6
```




/etc/sysctl.conf  
ту часть что отвечает за соединения :

#net.netfilter.nf_conntrack_tcp_timeout_established = 900 (default 432000 = 120 часов )
помнить сессию 5 дней если даже если по ней пробежал один байт

#для  работы GRE тунелей добавим  в debian stretch
net.netfilter.nf_conntrack_helper = 1

Проверим что бы были строки :

net.ipv4.ip_forward=1 # 

net.nf_conntrack_max=262144  #увеличение максимальноко колличества сейссий   conntrack_buckets * 4
                             #для избежания сообщений nf_conntrack: table full, dropping packet в dmesg 
                             #при ddos или flood от абонента

net.netfilter.nf_conntrack_buckets = 65536 

Сетевые параметры  ядра описаны на странице :
https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt
```
nf_conntrack_buckets - INTEGER
	Size of hash table. If not specified as parameter during module
	loading, the default size is calculated by dividing total memory
	by 16384 to determine the number of buckets but the hash table will
	never have fewer than 32 and limited to 16384 buckets. For systems
	with more than 4GB of memory it will be 65536 buckets.
	This sysctl is only writeable in the initial net namespace.
```

