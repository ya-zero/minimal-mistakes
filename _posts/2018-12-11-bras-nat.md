---
title: "Задача переноса отдельно стоящего nat сервера на  BRAS сервер "
date: 2018-12-11 09:00:00 +0000
categories:
  - NAT
tags:
  - accel-ppp
  - debian
  - persistent
sidebar:
  - title: "Debian NAT"
    text: "настройка nat"
---

На BRAS accel-ppp был отключен connection tracking для абонентских сетей, который нужен для NAT
```bash
#iptables -t raw -L -n
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  192.168.0.0/24       0.0.0.0/0           
ACCEPT     all  --  10.2.1.0/24          0.0.0.0/0           
ACCEPT     all  --  0.0.0.0/0            192.168.0.0/24      
ACCEPT     all  --  0.0.0.0/0            10.2.1.0/24         
NOTRACK    all  --  0.0.0.0/0            0.0.0.0/0           

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  0.0.0.0/0            192.168.0.0/24      
ACCEPT     all  --  0.0.0.0/0            10.2.1.0/24         
ACCEPT     all  --  192.168.0.0/24       0.0.0.0/0           
ACCEPT     all  --  10.2.1.0/24          0.0.0.0/0           
NOTRACK    all  --  0.0.0.0/0            0.0.0.0/0     
```
Нужно либо переписать правило NOTRACK all 0.0.0.0/0 на ACCEPT,  либо удалить его .  
Оптимальнее всего убрать все правила из цепочки RAW.

выполним через консоль:

```bash
iptables -t raw -F PREROUTING
iptables -t raw -F OUTPUT
```
и сохраним изменеия в скрипте который подгружает правила при старте. 

 
best practics https://fasterdata.es.net/host-tuning/linux/
best practics https://habr.com/company/mailru/blog/314168/


Если сервер работает только маршрутизатором, то тюнинг TCP стека особого значения не имеет, кроме тюнинга параметров касаемых производительносит сетевой карты (буфера, очередей, распределения прерывайни и т.д.)

Планируемая нагрузка будет в районе 2Gbit, стандартные параметры ядра на это не расчитаны, они выбраны средние 
для nat/web-server/gw.  Нам нужно все это оптимизировать под наши задачи.

Будет рассмотрена два этапа.
1) тюнинг сетевого стека
![]({{ site.baseurl }}/uploads/Figure-1-Data-receiving-process.jpg "nic tune")
https://opensourceforu.com/2016/10/network-performance-monitoring/

2) тюнинг работы системы касаемо cpu, а производительность самой системы нам не важа.
cpu governor

<h2>Этап 1</h2>
<h3> 1) Interrupt Coalescence  </h3>
После того как сетевой адаптер получает пакеты, драйвер устройства выдает одно жесткое прерывание, за которым следуют программные
прерывания (обрабатываются NAPI). Объединение прерываний - это количество пакетов, которые сетевой адаптер получает до выдачи
жесткого прерывания. Изменение значения для быстрого прерывания может привести к большим накладным расходам и, следовательно, к
снижению производительности, но высокое значение может привести к потере пакетов. По умолчанию ваши настройки находятся в режиме
адаптивной ИС, который автоматически балансирует значение в соответствии с сетевым трафиком. Но поскольку новые ядра используют
NAPI, что делает быстрые прерывания гораздо менее затратными (с точки зрения производительности), вы можете отключить эту
функцию.
```bash
ethtool -c eth0
ethtool -C eth0 adaptive-rx off

ethtool -c eth0
Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 0
sample-interval: 0
```

# 2) Пауза кадров 
Кадр паузы - это длительность паузы в миллисекундах, которую адаптер выдает сетевому коммутатору для остановки отправки пакетов, если кольцевой буфер заполнен. Если включен режим паузы, потеря пакетов будет минимальной.
```bash 
ethtool -a eth0
Pause parameters for eth0:
Autonegotiate:	off
RX:		off
TX:		off
ethtool -A eth0   tx on rx on
```


# 3)  изменение параметров через ethtool.  
Часть статей готоврит что сетевые карты затрудительно работают с этими включенными  параметрами. 
ethtool -K eth1 tx off rx off sg off tso off ufo off gso off lro off rxvlan off txvlan off

# 4)  Некоторые сетевые карты и их драйверы поддерживают 
настройку размера очереди приёма. Конкретная реализация зависит от оборудования, но, к счастью, ethtool обеспечивает стандартный 
метод настройки. Увеличение размера позволяет предотвратить отбрасывание сетевых данных при большом количестве входящих фреймов. 
Правда, данные ещё могут быть отброшены на уровне ПО, так что для полного исключения или снижения отбрасывания необходимо будет 
провести дополнительную настройку.

Проверка текущего размера очереди сетевой карты с помощью ethtool –g:

```bash
$ sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:   4096
RX Mini:  0
RX Jumbo: 0
TX:   4096
Current hardware settings:
RX:   512
RX Mini:  0
RX Jumbo: 0
TX:   512
```

Выходные данные показывают, что оборудование поддерживает 4096 дескрипторов приёма и передачи, но в данный момент используется 512.

Увеличим размер каждой очереди до 4096:
```bash 
$ sudo ethtool -G eth0 rx 4096
```
# 5)  количесвто очередей.    можно настривать разными способами.  
если сетевая карта поддерживает передачу параметров через
ethtool - l то вывод будет 
```bash
ethtool -l eth0
Channel parameters for eth0:
Pre-set maximums:
RX:		0
TX:		0
Other:		1
Combined:	63
Current hardware settings:
RX:		0
TX:		0
Other:		1
Combined:	24
```
Для того, чтобы равномерно распределить прерывания на сетевом интерфейсе необходимо изменить количество очередей.
Например, у нас имеется 6-ти ядерный процессор, а сетевая карта поддерживает 8 очередей.
(Очень важно! Для того чтобы установить именно то значение, которое требуется, необходимо сбросить значение количества очередей
до 1. Так как ethtool -L прибавляет количество очередей.http://docs.carbonsoft.ru/ )

```bash
ethtool -L eth1 rx 1 tx 1
ethtool -L eth1 rx 5 tx 5
```
Получаем (в поле Combined):

```bash
ethtool -l eth1
Channel parameters for eth1:
Pre-set maximums:
RX:		16
TX:		16
Other:		1
Combined:	16
Current hardware settings:
RX:		0
TX:		0
Other:		1
Combined:	6
```

# 6) распределение прерыванйи между ядрами  , 
в инете есть скрипт который привязывает прерывания к ядрам smp_affinity но можно сделать и вручную.


Предпочтительнее использовать прерывания MSI-X, особенно для сетевых карт, поддерживающих несколько очередей приёма. Причина в
том, что каждой очереди присвоено собственное аппаратное прерывание, которая может быть обработано конкретным CPU (с помощью
irqbalance или модифицирования /proc/irq/IRQ_NUMBER/smp_affinity). Как мы скоро увидим, прерывание и пакет обрабатывает один и
тот же CPU. Таким образом, входящие пакеты будут обрабатываться разными CPU в рамках всего сетевого стека, начиная с уровня
аппаратных прерываний(https://habr.com/company/mailru/blog/314168/).

При подготовке в эксплуатацию сервера с Intel Xeon E5520 (8 ядер, каждое с HyperThreading) я выбрал такую схему распределения
прерыванийв(зято с https://habr.com/post/108240/ :

Распределение вручную 

```bash

#CPU6
echo 40 > /proc/irq/71/smp_affinity
echo 40 > /proc/irq/84/smp_affinity

#CPU7
echo 80 > /proc/irq/72/smp_affinity
echo 80 > /proc/irq/85/smp_affinity

#CPU8
echo 100 > /proc/irq/73/smp_affinity
echo 100 > /proc/irq/86/smp_affinity

#CPU9
echo 200 > /proc/irq/74/smp_affinity
echo 200 > /proc/irq/87/smp_affinity

#CPU10
echo 400 > /proc/irq/75/smp_affinity
echo 400 > /proc/irq/80/smp_affinity

#CPU11
echo 800 > /proc/irq/76/smp_affinity
echo 800 > /proc/irq/81/smp_affinity

#CPU12
echo 1000 > /proc/irq/77/smp_affinity
echo 1000 > /proc/irq/82/smp_affinity

#CPU13
echo 2000 > /proc/irq/78/smp_affinity
echo 2000 > /proc/irq/83/smp_affinity

#CPU14
echo 4000 > /proc/irq/70/smp_affinity
#CPU15
echo 8000 > /proc/irq/79/smp_affinit
```
Распределение с помощью скрипта

я использую скрипт https://gist.github.com/SaveTheRbtz/8875474 с параметрами 

```bash
/usr/local/sbin/set_irq_affinity.sh -x 1,2,4,6,8,10,12,14,16,18,20,22 eth2
```
в моем случаея два процессора 6 ядерных по 2 потока , итого 24 потока .  привязыват очереди к разным процессорам нет смысла.
так как мы тогда  будет  еще и передавать часть пакеты принадлежащие к одному пакеты на разные процессоры .
NUMA node(s): 2
NUMA node0 CPU(s): 1,2,4,6,8,10,12,14,16,18,20,22
NUMA node1 CPU(s): 0,3,5,7,9,11,13,15,17,19,21,23

# UPD Все примеры привожу для карт построенных на чипах с поддержкой драйверов igb, ixgb.

# если мы не хотим разбираться со всем этим идем на https://habr.com/post/331720/  и скачиваем скрипт netutils-linux

Перейдем ко второй стадии нашего тининга сетевого стека изображенно на картинке выше.

Software interrupts


----
/etc/sysctl.conf  
ту часть что отвечает за соединения :

#net.netfilter.nf_conntrack_tcp_timeout_established = 900 (default 432000 = 120 часов )
помнить сессию 5 дней если даже если по ней пробежал один байт

#для  работы GRE тунелей добавим  в debian stretch
net.netfilter.nf_conntrack_helper = 1

Проверим что бы были строки :

net.ipv4.ip_forward=1 # 

net.nf_conntrack_max=262144  #увеличение максимальноко колличества сейссий   conntrack_buckets * 4
                             #для избежания сообщений nf_conntrack: table full, dropping packet в dmesg 
                             #при ddos или flood от абонента

net.netfilter.nf_conntrack_buckets = 65536 

Сетевые параметры  ядра описаны на странице :
https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt
```
nf_conntrack_buckets - INTEGER
	Size of hash table. If not specified as parameter during module
	loading, the default size is calculated by dividing total memory
	by 16384 to determine the number of buckets but the hash table will
	never have fewer than 32 and limited to 16384 buckets. For systems
	with more than 4GB of memory it will be 65536 buckets.
	This sysctl is only writeable in the initial net namespace.
```

